{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6MMRRFa_YbU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generate data\n",
        "np.random.seed(42)\n",
        "X = np.linspace(-5, 5, 100).reshape(-1, 1)\n",
        "y_true = 2 * X.flatten() + 3  # Linear function: y = 2x + 3\n",
        "y = y_true + np.random.normal(0, 2, size=X.shape[0])  # Add Gaussian noise\n",
        "\n",
        "# Split into training and testing sets (80:20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Function to fit and evaluate models\n",
        "def fit_and_evaluate(degree):\n",
        "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Degree {degree} - MSE: {mse:.4f}\")\n",
        "    return model\n",
        "\n",
        "# Fit models with different polynomial degrees\n",
        "degrees = [1, 2, 3]\n",
        "models = {d: fit_and_evaluate(d) for d in degrees}\n",
        "\n",
        "# Plot results\n",
        "plt.scatter(X, y, color='gray', label='Data', alpha=0.5)\n",
        "x_range = np.linspace(-5, 5, 100).reshape(-1, 1)\n",
        "colors = ['r', 'g', 'b']\n",
        "labels = ['Linear', 'Quadratic', 'Cubic']\n",
        "\n",
        "for i, d in enumerate(degrees):\n",
        "    y_range_pred = models[d].predict(x_range)\n",
        "    plt.plot(x_range, y_range_pred, colors[i], label=f'{labels[i]} Fit')\n",
        "\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.title(\"Polynomial Regression Fits\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"p2.csv\"  # Update with the correct path if needed\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Extract features and target variable\n",
        "X = df[['Feature 1', 'Feature 2']].values  # Features\n",
        "y = df['Output'].values  # Target\n",
        "\n",
        "# Normalize features for better convergence\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "\n",
        "# Add bias term (intercept) to X\n",
        "X = np.c_[np.ones(X.shape[0]), X]\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Compute Mean Squared Error\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "# Logistic Regression using Gradient Descent\n",
        "def logistic_regression(X, y, lr, iterations):\n",
        "    m, n = X.shape\n",
        "    weights = np.zeros(n)  # Initialize weights\n",
        "    mse_values = []\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        z = np.dot(X, weights)\n",
        "        y_pred = sigmoid(z)\n",
        "\n",
        "        # Compute gradient\n",
        "        gradient = np.dot(X.T, (y_pred - y)) / m\n",
        "\n",
        "        # Update weights\n",
        "        weights -= lr * gradient\n",
        "\n",
        "        # Compute MSE for monitoring\n",
        "        mse_values.append(mse(y, y_pred))\n",
        "\n",
        "    return weights, mse_values\n",
        "\n",
        "# Train the model for both learning rates\n",
        "iterations = 20\n",
        "learning_rates = [0.01, 0.05]\n",
        "results = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    weights, mse_values = logistic_regression(X, y, lr, iterations)\n",
        "    results[lr] = (weights, mse_values)\n",
        "    print(f\"Learning Rate: {lr}\\nFinal Weights: {weights}\\n\")\n",
        "\n",
        "# Plot MSE variation for both learning rates\n",
        "plt.figure(figsize=(10, 5))\n",
        "for lr in learning_rates:\n",
        "    plt.plot(range(1, iterations + 1), results[lr][1], label=f'LR = {lr}')\n",
        "\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.title('MSE Variation Over Iterations')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JzUG8R0ABcUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv('p3.csv')\n",
        "\n",
        "# Assuming the last column is the target variable\n",
        "y = data.iloc[:, -1]\n",
        "X = data.iloc[:, :-1]\n",
        "\n",
        "# Split into training (80%) and testing (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "lr = LinearRegression()\n",
        "lasso = Lasso(alpha=2)  # lambda/2 = 1, so alpha = 2\n",
        "ridge = Ridge(alpha=0.2)  # lambda/2 = 0.1, so alpha = 0.2\n",
        "\n",
        "# Train models\n",
        "lr.fit(X_train, y_train)\n",
        "lasso.fit(X_train, y_train)\n",
        "ridge.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_lr = lr.predict(X_test)\n",
        "y_pred_lasso = lasso.predict(X_test)\n",
        "y_pred_ridge = ridge.predict(X_test)\n",
        "\n",
        "# Compute MSE\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
        "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
        "\n",
        "# Bar plot for MSE comparison\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(['Linear', 'LASSO', 'Ridge'], [mse_lr, mse_lasso, mse_ridge], color=['blue', 'red', 'green'])\n",
        "plt.ylabel('MSE')\n",
        "plt.title('MSE Comparison')\n",
        "plt.show()\n",
        "\n",
        "# Bar plot for feature coefficients\n",
        "coeffs = pd.DataFrame({'Feature': X.columns,\n",
        "                        'Linear': lr.coef_,\n",
        "                        'LASSO': lasso.coef_,\n",
        "                        'Ridge': ridge.coef_})\n",
        "\n",
        "coeffs.set_index('Feature').plot(kind='bar', figsize=(10, 6))\n",
        "plt.ylabel('Coefficient Value')\n",
        "plt.title('Feature Coefficients Comparison')\n",
        "plt.show()\n",
        "\n",
        "# Print MSE values\n",
        "print(f\"Linear Regression MSE: {mse_lr}\")\n",
        "print(f\"LASSO Regression MSE: {mse_lasso}\")\n",
        "print(f\"Ridge Regression MSE: {mse_ridge}\")\n"
      ],
      "metadata": {
        "id": "KKPSYL82Bk1O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}