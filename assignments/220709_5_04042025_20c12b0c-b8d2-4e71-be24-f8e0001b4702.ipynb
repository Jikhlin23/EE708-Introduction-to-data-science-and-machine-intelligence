{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5lJDkw-GISt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, learning_rate=0.01, iterations=20):\n",
        "        self.lr = learning_rate\n",
        "        self.iterations = iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def step_function(self, x):\n",
        "        return np.where(x >= 0, 1, 0)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        np.random.seed(42)  # For reproducibility\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize weights randomly\n",
        "        self.weights = np.random.randn(n_features)\n",
        "        self.bias = np.random.randn()\n",
        "\n",
        "        for _ in range(self.iterations):\n",
        "            for idx, x_i in enumerate(X):\n",
        "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
        "                y_predicted = self.step_function(linear_output)\n",
        "\n",
        "                # Update rule\n",
        "                update = self.lr * (y[idx] - y_predicted)\n",
        "                self.weights += update * x_i\n",
        "                self.bias += update\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_output = np.dot(X, self.weights) + self.bias\n",
        "        return self.step_function(linear_output)\n",
        "\n",
        "\n",
        "def plot_decision_boundary(X, y, model):\n",
        "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, 0.1),\n",
        "                           np.arange(x2_min, x2_max, 0.1))\n",
        "\n",
        "    grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
        "    predictions = model.predict(grid).reshape(xx1.shape)\n",
        "\n",
        "    plt.contourf(xx1, xx2, predictions, alpha=0.3, cmap=plt.cm.coolwarm)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolor='k')\n",
        "    plt.xlabel('X1')\n",
        "    plt.ylabel('X2')\n",
        "    plt.title('Decision Boundary')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load dataset\n",
        "    data = pd.read_csv('A5_P1.csv')\n",
        "    X = data[['X1', 'X2']].values\n",
        "    y = data['y'].values\n",
        "\n",
        "    # Initialize and train perceptron\n",
        "    perceptron = Perceptron(learning_rate=0.01, iterations=20)\n",
        "    perceptron.fit(X, y)\n",
        "\n",
        "    # Plot decision boundary\n",
        "    plot_decision_boundary(X, y, perceptron)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Activation functions and their derivatives\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.tanh(x) ** 2\n",
        "\n",
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "# Loss function: Mean Squared Error\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "def mse_derivative(y_true, y_pred):\n",
        "    return (y_pred - y_true)\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_onehot = np.zeros((y.size, y.max() + 1))\n",
        "y_onehot[np.arange(y.size), y] = 1\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Network architecture\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 5\n",
        "output_size = 3\n",
        "\n",
        "# Initialize weights and biases\n",
        "np.random.seed(42)\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.random.randn(hidden_size)\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.random.randn(output_size)\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "# Lists to store loss and accuracy values\n",
        "train_losses, test_losses = [], []\n",
        "train_accuracies, test_accuracies = [], []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Forward propagation (Training)\n",
        "    Z1 = np.dot(X_train, W1) + b1\n",
        "    A1 = tanh(Z1)\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = softmax(Z2)\n",
        "\n",
        "    # Calculate loss and accuracy for training\n",
        "    train_loss = mse(y_train, A2)\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracy = np.mean(np.argmax(A2, axis=1) == np.argmax(y_train, axis=1))\n",
        "    train_accuracies.append(train_accuracy)\n",
        "\n",
        "    # Backward propagation\n",
        "    dZ2 = mse_derivative(y_train, A2)\n",
        "    dW2 = np.dot(A1.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0)\n",
        "\n",
        "    dA1 = np.dot(dZ2, W2.T)\n",
        "    dZ1 = dA1 * tanh_derivative(Z1)\n",
        "    dW1 = np.dot(X_train.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0)\n",
        "\n",
        "    # Update weights and biases\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "\n",
        "    # Testing\n",
        "    Z1_test = np.dot(X_test, W1) + b1\n",
        "    A1_test = tanh(Z1_test)\n",
        "    Z2_test = np.dot(A1_test, W2) + b2\n",
        "    A2_test = softmax(Z2_test)\n",
        "\n",
        "    # Calculate loss and accuracy for testing\n",
        "    test_loss = mse(y_test, A2_test)\n",
        "    test_losses.append(test_loss)\n",
        "    test_accuracy = np.mean(np.argmax(A2_test, axis=1) == np.argmax(y_test, axis=1))\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Train Acc: {train_accuracy:.4f}, Test Acc: {test_accuracy:.4f}')\n",
        "\n",
        "# Plotting Loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(test_losses, label='Testing Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Loss vs. Epochs')\n",
        "\n",
        "# Plotting Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_accuracies, label='Training Accuracy')\n",
        "plt.plot(test_accuracies, label='Testing Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.title('Accuracy vs. Epochs')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "bt-cUlH1HRF-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}